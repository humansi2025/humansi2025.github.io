<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HumanSI Challenge 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>
<body>
    <header>
        <div class="logo-container">
            <img src="images/humansi_logo_v2.png" alt="HumanSI Challenge Logo" class="logo">
        </div>
        <h1>HumanSI Challenge 2025</h1>
    </header>

    <nav>
        <ul>
            <li><a href="#important-dates">Important Dates</a></li>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#task">Challenge Tracks</a></li>
            <li><a href="#prizes">Prizes</a></li>
            <!-- <li><a href="#workshop">Workshop</a></li> -->
            <li><a href="#organizers">Organizers</a></li>
            <!-- <li><a href="#speakers">Speakers</a></li>
            <li><a href="#impact">Impact</a></li> -->
        </ul>
    </nav>

    <main>
        <section id="important-dates" class="section">
            <h2>Important Dates</h2>
            <ul class="dates-list">
                <li><strong>Registration Deadline:</strong> July 5, 2025, 24:00 (UTC+8)</li>
                <li><strong>Submission Deadline:</strong> July 10, 2025, 24:00 (UTC+8)</li>
                <li><strong>Results Notification:</strong> July 13, 2025</li>
                <li><strong>On-site Report:</strong> July 18, 2025</li>
            </ul>
        </section>

        <section id="overview" class="section">
            <h2>Challenge Overview</h2>
            <!-- <p>Welcome to the 2025 Object3DD Challenge, a premier international competition focused on advancing traffic scene understanding through 3D object detection. This challenge is part of the 13th International Conference on Mobile Mapping Technology (MMT 2025) and aims to bring together researchers and practitioners to tackle fundamental challenges in object perception under autonomous driving scenarios.</p>
            
            <p>The significance of this research domain is multifaceted:</p>
            <ol>
                <li>Advancing autonomous vehicle perception systems through robust 3D object perception.</li>
                <li>Enabling knowledge transfer across different mechanisms to enhance the generalizability of autonomous driving models under various mechanisms of LiDAR sensors.</li>
                <li>Developing multimodal collaborative perception systems to improve the safety of autonomous driving.</li>
            </ol>
            
            <p>With the continuous iteration of 3D sensors and the widespread adoption of intelligent vehicles, there is an urgent need to develop frameworks for cross-sensor LiDAR and multi-agent collaborative object perception. In this challenge, we welcome submissions of efficient domain adaptation methods and multimodal collaborative perception approaches. This workshop provides a structured platform for the dissemination of algorithmic innovations, methodological advances, and empirical findings in this rapidly evolving field. The challenge is open to students, teachers, and researchers in relevant fields.</p> -->
            <p>In the era of deep integration of artificial intelligence and 3D perception technologies, multimodal human pose estimation (HPE), as a key technology bridging the physical and digital worlds, is becoming a core enabler in cutting-edge fields such as embodied intelligent interaction, metaverse construction, motion analysis, and autonomous systems. Current HPE methods and datasets primarily remain at the level of monocular cameras, while 3D vision technology is not only iterating on single sensors but also requires innovation and breakthroughs in multi-sensor fusion technologies. To bring together top global research efforts and explore the boundaries of high-precision multimodal 3D human pose estimation and global trajectory tracking in complex dynamic environments, we are hosting the HumanSI (Human Spatial Intelligence) Multimodal Human Pose Estimation Challenge.</p>
            <p>This challenge is based on the RELI11D dataset and the LEIR method published at the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). It aims to inspire participants to develop and optimize algorithms for high-precision 3D human pose estimation using multimodal data (RGB, LiDAR, IMU motion capture sensors, and event cameras). The competition provides a platform for participating teams to test their technical prowess in interdisciplinary fields such as computer vision, deep learning, and multimodal fusion, while driving significant advancements in real-time motion capture, high-fidelity digital twins, and automated scene understanding.</p>
            <p>If you are interested in this challenge, please register via the QR code or link below. For any questions during the registration process, please contact the challenge organizer.</p>
            <p><strong>Registration Link: </strong><a href="https://www.wjx.top/vm/wPriU4P.aspx#">https://www.wjx.top/vm/wPriU4P.aspx#</a></p>
            <br><div style="text-align: center;">
                <img src="images/QRcode.png" class="center">
            </div>
        </section>

        <section id="tracks" class="section">
            <h2>Challenge Task (Submission Specifications and Evaluation Criteria)</h2>
            
            <!-- <div class="track"> -->
                <h3><strong>Recommended Hardware</strong></h3>
                <ul>
                    <li><strong>CPU:</strong> Intel Xeon Gold 6248</li>
                    <li><strong>GPU:</strong> NVIDIA RTX 4090</li>
                    <li><strong>RAM:</strong> 128GB DDR4</li>
                </ul>

                <h3><strong>Core Evaluation Dimensions (Threshold Conditions)</strong></h3>
                <ul>
                    <p><strong>Overall Requirement:</strong></p>
                    <li><strong>MPJPE ≤ 100 mm, PA-MPJPE ≤ 90 mm</strong></li>
                </ul>

                <h3><strong>Specialized Pose Estimation Metrics (Based on RELI11D Paper)</strong></h3>
                <ul>
                    <li><strong>ACCEL:</strong> Measures the accuracy of human motion acceleration prediction (unit: m/s², ↓). This metric evaluates the algorithm's performance in capturing dynamic movements (e.g., rapid acceleration changes), particularly suitable for fast and complex motion scenarios such as table tennis or taekwondo.</li>
                    <li><strong>MPJPE (Mean Per-Joint Position Error):</strong> The average Euclidean distance between predicted 3D joint positions and ground truth (unit: mm, ↓). This metric directly assesses pose estimation accuracy.</li>
                    <li><strong>PA-MPJPE (Procrustes-Aligned MPJPE):</strong> MPJPE after global rotation and translation alignment (unit: mm, ↓). This metric eliminates the influence of global transformations, focusing on local joint pose accuracy.</li>
                    <li><strong>PVE (Position and Velocity Error):</strong> A comprehensive evaluation of 3D joint position and motion velocity prediction errors (unit: mm, ↓). This metric is particularly useful for assessing the smoothness and consistency of global trajectories.</li>
                    <li><strong>PCK0.3 (Percentage of Correct Keypoints within 0.3m):</strong> The proportion of correctly predicted joints within a 0.3-meter threshold (↑ higher is better). This metric evaluates algorithm robustness under larger error tolerances, suitable for pose estimation in complex scenarios.</li>
                </ul>

                <h3><strong>RELI11D Dataset Benchmark & Submission & Presentation</strong></h3>
                <ul>
                    <p><strong>RELI11D Dataset Benchmark:</strong></p>
                    <p><strong>Input:</strong> <br>

                    Unspecified number of: nspecified number of: <br>

                    <li>RGB sequences</li>
                    <li>LiDAR point clouds</li>
                    <li>IMU measurements</li>
                    <li>Event camera streams</li><br>

                    (Participants may freely choose the input modalities they wish to use. The organizers recommend using RGB + LiDAR + Event modalities.)</p>
                    
                    <p><strong>Output Requirements:</strong>  <br>

                    3D joint positions, SMPL parameters (θ, β, T), Global trajectory (optional) <br>

                    (Output format should follow the RELI11D dataset standard.)</p>
    
                    <p><strong>Optional Scene Data:</strong> <br>

                    The dataset includes the following high-precision scene data for physics-based human pose estimation:

                    <li>Lidar-scanned 3D environments</li>                        
                    <li>High-density mesh scenes</li>    
                    <li>Sparse rotating radar scenes</li>
                </p>
                </ul><br>

                <ul>
                    <p><strong>Submission:</strong></p>
                    <p><strong>Required Output:</strong> <br>

                    Participants must process the <strong>test_upload</strong> folder (no ground truth provided) and generate:<br>

                    <li>3D joint positions</li>
                    <li>SMPL parameters (θ, β, T)</li>
                    <li>Global trajectory (optional)</li><br>
                    
                    Format: Strictly follow the RELI11D dataset specifications.</p>

                    <p><strong>Code Submission:</strong>  <br>
                    
                    Runnable scripts / executable programs / source code (must be functional).</p>
                    
                    <p><strong>Technical Report (≤5 pages):</strong> 
                    
                    <li>Method overview</li>
                    
                    <li>Key innovations</li>
                    
                    <li>Algorithm runtime & resource usage (memory/GPU consumption)</li></p>

                    <p><strong>Email all materials to the technical support email (address to be specified).</strong></p>

                </ul>
                    
                <ul>
                    <p><strong>Presentation:</strong><br>
                        Format:<br>
                        15-minute algorithm presentation with test set pose estimation demo + 10-minute Q&A with judges
                    </p>

                </ul>

                <h3><strong>Additional Resources</strong></h3>
                <ul>
                    <li><strong>Download the RELI11D Challenge Version Dataset:</strong></li><br>
                    Link: <a href="https://pan.quark.cn/s/cc337968e054" target="_blank"> https://pan.quark.cn/s/cc337968e054</a><br>
                    Access Code: A2v4 <br>

                    <li><strong>Download the RELI11D Challenge Version Test Dataset for Upload Submissions:</strong></li><br>
                    Link: <a href="https://pan.quark.cn/s/d66f10a0f36d " target="_blank"> https://pan.quark.cn/s/d66f10a0f36d </a><br>
                    Access Code: VHva<br>

                    <li><strong>RELI11D Project Page:</strong><a href="http://www.lidarhumanmotion.net/reli11d/" target="_blank"> http://www.lidarhumanmotion.net/reli11d/</a></li>
                    <li><strong>RELI11D & LEIR Algorithm Page:</strong><a href="https://github.com/yanmn/RELI11DandLIER" target="_blank"> https://github.com/yanmn/RELI11DandLIER</a></li>
                    <li><strong>RELI11D Dataset Tips:</strong><a href="http://www.lidarhumanmotion.net/data-reli11d/" target="_blank">  http://www.lidarhumanmotion.net/data-reli11d/</a></li>
                    <li><strong>RELI11D Paper:</strong><a style="font-size: smaller;" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_RELI11D_A_Comprehensive_Multimodal_Human_Motion_Dataset_and_Method_CVPR_2024_paper.pdf" target="_blank"> https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_RELI11D_A_Comprehensive_Multimodal_Human_Motion_Dataset_and_Method_CVPR_2024_paper.pdf</a></li>
                </ul>

                <h3><strong>Competition Requirements</strong></h3>
                <ul>
                    <p>Participants from enterprises, institutions, universities (including faculty, students, and teams), as well as individuals nationwide, are eligible to compete. (Team size should not exceed five members.)</p>    
                </ul>
            <!-- </div> -->
        </section>

        <section id="prizes" class="section">
            <h2>Prizes</h2>
            <p>We are thankful to our sponsor for providing the prizes. The prize award will be granted to the Top 3 individuals and teams on the leaderboard that provide valid submissions.</p>
            <ul class="prizes-list">
<!--                 <li><strong>1st Place:</strong> XXX CNY</li>
                <li><strong>2nd Place:</strong> XXX CNY</li>
                <li><strong>3rd Place:</strong> XXX CNY</li> -->
            </ul>
        </section>

        <!-- <section id="workshop" class="section">
            <h2>Workshop Format</h2>
            <p>The workshop will implement a hybrid participation model, accommodating both in-person attendance and virtual engagement to maximize accessibility and international participation. The programmatic structure will comprise:</p>
            <ul>
                <li>Invited keynote presentations from recognized domain experts</li>
                <li>Formal recognition ceremonies for competition winners</li>
                <li>Technical presentations from winning teams detailing their methodological approaches</li>
                <li>A structured panel discussion addressing emerging research directions</li>
            </ul>

            <h3>Schedule</h3>
            <table class="schedule-table">
                <tr>
                    <th>Time</th>
                    <th>Event</th>
                </tr>
                <tr>
                    <td>14:00-14:05</td>
                    <td>Welcome Introduction</td>
                </tr>
                <tr>
                    <td>14:05-14:35</td>
                    <td>Invited Talk (Talk 1)</td>
                </tr>
                <tr>
                    <td>14:35-15:05</td>
                    <td>Invited Talk (Talk 2)</td>
                </tr>
                <tr>
                    <td>15:05-15:35</td>
                    <td>Coffee break</td>
                </tr>
                <tr>
                    <td>15:35-15:50</td>
                    <td>Awarding Ceremony</td>
                </tr>
                <tr>
                    <td>15:50-16:10</td>
                    <td>Winner Talk (Track 1) + Q&A</td>
                </tr>
                <tr>
                    <td>16:10-16:40</td>
                    <td>Winner Talk (Track 2) + Q&A</td>
                </tr>
                <tr>
                    <td>16:40-17:20</td>
                    <td>Panel Discussion</td>
                </tr>
                <tr>
                    <td>17:20-17:30</td>
                    <td>Closing Remarks</td>
                </tr>
            </table>
            
        </section> -->

        <section id="organizers" class="section">
            <h2>Organizing Committee</h2>

            <h3>Organizer</h3>
            <div class="organizer">
                <p><strong>Chinese Society of Image and Graphics (CSIG)</strong>
            </div>

            <h3>Host</h3>
            <div class="organizer">
                <p><strong>Xiamen University</strong>
            </div>
            
            <h3>Primary Organizer</h3>
            <div class="organizer">
                <p><strong>Siqi Shen:</strong> Associate Professor and doctoral supervisor in the Department of Computer Science, School of Information Science and Engineering, Xiamen University. he is mainly engaged in research in the fields of multi-agent perception and computing (multi-agent reinforcement learning, human motion and scene capture and generation, large models, AI Agents, visual positioning, embodied intelligence, etc.).
                <br><a href="siqishen@xmu.edu.cn">siqishen@xmu.edu.cn</a></p>
            </div>

            <h3>Co-Organizers</h3>
            <div class="organizer">
                <p><strong>Ming Yan:</strong> Ph.D. Student, ASC, Xiamen University. His research interests lie in sports-based complex human motion recovery.
                <br><a href="yanmnn@stu.xmu.edu.cn">yanmnn@stu.xmu.edu.cn</a></p>
            </div>
            <div class="organizer">
                <p><strong>Yuhua Luo:</strong> Ph.D. Student, ASC, Xiamen University. His research focuses on 3D human pose estimation and computer vision.
                <br><a href="luoyh@stu.xmu.edu.cn">luoyh@stu.xmu.edu.cn</a></p>
            </div>
            <div class="organizer">
                <p><strong>Mengyin Liu:</strong> M.S. Student, ASC, Xiamen University. Her research interests include 3D computer vision and human pose estimation.
                <br><a href="liumengyin@stu.xmu.edu.cn">liumengyin@stu.xmu.edu.cn</a></p>
            </div>
            <div class="organizer">
                <p><strong>Shuqi Fan:</strong> M.S. Student, ASC, Xiamen University. Her research interests include 3D computer vision and human pose estimation.
                <br><a href="fanshuqi@stu.xmu.edu.cn">fanshuqi@stu.xmu.edu.cn</a></p>
            </div> 
            <div class="organizer">
                <p><strong>Zekai Wu:</strong> M.S. Student, ASC, Xiamen University. His research interests center on leveraging event cameras for human pose estimation within the domain of 3D computer vision.
                <br><a href="23020241154456@stu.xmu.edu.cn">23020241154456@stu.xmu.edu.cn</a></p>
            </div> 
<!--             <div class="organizer">
                <p><strong>Hai Wu:</strong> Assistant Researcher, Pengcheng Lab. His research interests include 3D computer vision and their applications in intelligent transportation systems.</p>
            </div> 
            <div class="organizer">
                <p><strong>Jinghao Deng:</strong> Algorithm Engineer, Xiaopeng. His research interests include 3D computer vision and their applications in intelligent transportation systems.</p>
            </div>  -->
        </section>

        <!-- <section id="speakers" class="section">
            <h2>Confirmed Speakers</h2>
            <div class="speaker">
                <p><strong>Hai Wu:</strong> Assistant Researcher, Pengcheng Lab. Topic: Research on High-Precision 3D Object Detection Algorithms.
 </p>
            </div>
        </section> -->

        <!-- <section id="impact" class="section">
            <h2>Broader Impact</h2>
            <p>The intellectual contributions and methodological frameworks developed through this challenge have the potential to catalyze significant technological and societal advancements across multiple domains:</p>
            <ul>
                <li>Enhanced Autonomous Driving Safety and Efficiency. Improving the accuracy and robustness of 3D object detection in autonomous vehicles enhances safety by enabling better perception in diverse environments. These technologies also optimize traffic flow management through more precise traffic information, reducing congestion and improving overall transportation efficiency.</li>
                <li>Improved Infrastructure Management and Environmental Sustainability. Accurate detection of infrastructure damage facilitates more efficient maintenance scheduling, reducing the risk of accidents caused by deteriorating infrastructure. Additionally, optimized traffic flow management lowers energy consumption and carbon emissions, supporting environmental sustainability.</li>
                <li>Advancements in Multimodal Data Fusion and System Integration. Providing New Frameworks for Data Integration. New frameworks for aligning and integrating data from multiple sensors enhance the capabilities of intelligent transportation systems. These advancements support the development of more comprehensive and reliable traffic monitoring and management solutions.</li>
            </ul>

            <h3>Ethical Considerations</h3>
            <p>The datasets utilized in this challenge have been collected and annotated in strict accordance with applicable privacy legislation and regulatory frameworks. All personally identifiable information has been methodically anonymized to ensure the protection of individual privacy rights and community interests. The organizing committee will implement rigorous protocols to ensure that the dataset utilization remains exclusively within the intended research domain of point cloud-based traffic scene understanding.</p>
            
            <p>For more information, please contact the organizing committee at xiaqiming@stu.xmu.edu.cn</p>
        </section> -->
    </main>

    <footer>
        <p>&copy; 2025 HumanSI Challenge. All rights reserved.</p>
    </footer>
</body>
</html>
